{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import jaccard_score, f1_score, accuracy_score\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        # ... (Your UNet model code) ...\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        # ... (Your custom dataset code) ...\n",
    "\n",
    "# Create a U-Net model with 1 input channel and 3 output channels for segmentation\n",
    "model = UNet(in_channels=1, out_channels=3)\n",
    "print(model)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load and preprocess your input image\n",
    "image_path = './Datasets/Dataset 1/data/BMMC_1.tif'\n",
    "image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "input_image = transform(image).unsqueeze(0)  # Add a batch dimension\n",
    "print(\"Input shape:\", input_image.shape)\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "# Define data directories\n",
    "image_dir = './Datasets/Dataset 1/data'\n",
    "mask_dir = './Datasets/Dataset 1/masks'\n",
    "\n",
    "# Define a transform to convert images and masks to tensors\n",
    "data_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Create a custom dataset\n",
    "dataset = CustomDataset(image_dir, mask_dir, transform=data_transform)\n",
    "\n",
    "# Create a dataloader\n",
    "batch_size = 1  # Adjust as needed\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define a loss function (e.g., cross-entropy) and an optimizer (e.g., Adam)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, masks in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs, masks = inputs.to(device), masks.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] Current Loss: {loss.item()}\")\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Average Loss: {running_loss / len(dataloader)}\")\n",
    "    print()\n",
    "    \n",
    "print(\"Training finished\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"unet_segmentation_model.pth\")\n",
    "\n",
    "# Evaluation and Testing\n",
    "model.eval()\n",
    "\n",
    "# Evaluate the model\n",
    "total_loss = 0.0\n",
    "num_samples = 0\n",
    "\n",
    "for inputs, masks in dataloader:  # Use a separate dataloader for evaluation\n",
    "    with torch.no_grad():\n",
    "        inputs, masks = inputs.to(device), masks.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, masks)\n",
    "        total_loss += loss.item()\n",
    "        num_samples += inputs.size(0)\n",
    "\n",
    "average_loss = total_loss / num_samples\n",
    "print(f\"Average Evaluation Loss: {average_loss}\")\n",
    "\n",
    "# Create and save sample segmentation outputs\n",
    "model.eval()\n",
    "\n",
    "# Lists to store evaluation results for each sample\n",
    "iou_scores = []\n",
    "dice_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "# Select a few images from the evaluation dataset\n",
    "num_samples_to visualize = 3\n",
    "visualize_dataloader = DataLoader(dataset, batch_size=num_samples_to_visualize, shuffle=True)\n",
    "\n",
    "for inputs, masks in visualize_dataloader:\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        # Assuming outputs are class probabilities, you can obtain the predicted class\n",
    "        predicted_masks = outputs.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        for i in range(num_samples_to_visualize):\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.title(\"Input Image\")\n",
    "            plt.imshow(inputs[i][0].cpu().numpy(), cmap='gray')\n",
    "\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.title(\"Ground Truth Mask\")\n",
    "            plt.imshow(masks[i][0].cpu().numpy(), cmap='jet')\n",
    "\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.title(\"Predicted Mask\")\n",
    "            plt.imshow(predicted_masks[i], cmap='jet')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            # Calculate and store evaluation metrics for each sample\n",
    "            iou = jaccard_score(masks[i][0].cpu().numpy().flatten(), predicted_masks[i].flatten())\n",
    "            dice = f1_score(masks[i][0].cpu().numpy().flatten(), predicted_masks[i].flatten())\n",
    "            accuracy = accuracy_score(masks[i][0].cpu().numpy().flatten(), predicted_masks[i].flatten())\n",
    "\n",
    "            iou_scores.append(iou)\n",
    "            dice_scores.append(dice)\n",
    "            accuracy_scores.append(accuracy)\n",
    "\n",
    "# Calculate and print average metrics\n",
    "average_iou = sum(iou_scores) / len(iou_scores)\n",
    "average_dice = sum(dice_scores) / len(dice_scores)\n",
    "average_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "\n",
    "print(f\"Average IoU (Jaccard Index): {average_iou}\")\n",
    "print(f\"Average Dice Coefficient: {average_dice}\")\n",
    "print(f\"Average Accuracy: {average_accuracy}\")\n",
    "\n",
    "print(\"Evaluation and Testing Finished\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
