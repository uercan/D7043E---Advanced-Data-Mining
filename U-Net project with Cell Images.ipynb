{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (connect): Sequential(\n",
      "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (final): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.channels_list = [64, 128, 256, 512]\n",
    "        self.layers = len(self.channels_list)\n",
    "\n",
    "        # Contracting Part\n",
    "        self.contracting_conv_blocks = []\n",
    "        self.max_pooling_blocks = []\n",
    "        for channels in self.channels_list:\n",
    "            self.contracting_conv_blocks.append(self.conv_block(in_channels, channels))\n",
    "            self.max_pooling_blocks.append(nn.MaxPool2d(2))\n",
    "            in_channels = channels\n",
    "\n",
    "        # Connecting Part\n",
    "        self.connect = self.conv_block(\n",
    "            self.channels_list[-1], 2 * self.channels_list[-1]\n",
    "        )\n",
    "\n",
    "        # Expansive Part\n",
    "        in_channels = 1024\n",
    "        self.expansive_conv_blocks = []\n",
    "        self.up_conv_blocks = []\n",
    "        for channels in reversed(self.channels_list):\n",
    "            self.expansive_conv_blocks.append(self.conv_block(in_channels, channels))\n",
    "            self.up_conv_blocks.append(\n",
    "                nn.ConvTranspose2d(in_channels, channels, 2, stride=2)\n",
    "            )\n",
    "            in_channels = channels\n",
    "\n",
    "        # Final Part\n",
    "        self.final = nn.Conv2d(self.channels_list[0], out_channels, 1, padding=0)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=0),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Contracting Part\n",
    "        encoder_outputs = []\n",
    "        for layer in range(self.layers):\n",
    "            conv_block = self.contracting_conv_blocks[layer]\n",
    "            x = conv_block(x)\n",
    "            encoder_outputs.append(x)\n",
    "            # TODO: Check if the 'x' object is modified or uniquely generated\n",
    "            max_pool = self.max_pooling_blocks[layer]\n",
    "            x = max_pool(x)\n",
    "\n",
    "        # Connecting Part\n",
    "        x = self.connect(x)\n",
    "\n",
    "        # Expansive Part\n",
    "        for reversed_layer in reversed(range(self.layers)):\n",
    "            layer = self.layers - reversed_layer - 1\n",
    "            up_conv_block = self.up_conv_blocks[layer]\n",
    "            x = up_conv_block(x)\n",
    "            # Concatenate with the corresponding cropped feature map\n",
    "            x = torch.cat((x, encoder_outputs[reversed_layer]), dim=1)\n",
    "            conv_block = self.expansive_conv_blocks[layer]\n",
    "            x = conv_block(x)\n",
    "\n",
    "        # Final Part\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create a U-Net model with 1 input channel and 2 output channels for binary segmentation\n",
    "model = UNet(in_channels=1, out_channels=2)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 112 but got size 120 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Docs\\Sweden\\Study\\Amir\\LTU\\Applied AI (Master)\\Courses\\D7043E - Advanced-Data-Mining\\U-Net project with Cell Images.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Docs/Sweden/Study/Amir/LTU/Applied%20AI%20%28Master%29/Courses/D7043E%20-%20Advanced-Data-Mining/U-Net%20project%20with%20Cell%20Images.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Forward pass through the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Docs/Sweden/Study/Amir/LTU/Applied%20AI%20%28Master%29/Courses/D7043E%20-%20Advanced-Data-Mining/U-Net%20project%20with%20Cell%20Images.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Docs/Sweden/Study/Amir/LTU/Applied%20AI%20%28Master%29/Courses/D7043E%20-%20Advanced-Data-Mining/U-Net%20project%20with%20Cell%20Images.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     output \u001b[39m=\u001b[39m model(input_image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Docs/Sweden/Study/Amir/LTU/Applied%20AI%20%28Master%29/Courses/D7043E%20-%20Advanced-Data-Mining/U-Net%20project%20with%20Cell%20Images.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# At this point, 'output' contains the model's prediction for the input image.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Docs/Sweden/Study/Amir/LTU/Applied%20AI%20%28Master%29/Courses/D7043E%20-%20Advanced-Data-Mining/U-Net%20project%20with%20Cell%20Images.ipynb#W1sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# You can post-process the output as needed for your specific task.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Docs\\Sweden\\Study\\Amir\\LTU\\Applied AI (Master)\\Courses\\D7043E - Advanced-Data-Mining\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Docs\\Sweden\\Study\\Amir\\LTU\\Applied AI (Master)\\Courses\\D7043E - Advanced-Data-Mining\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Docs\\Sweden\\Study\\Amir\\LTU\\Applied AI (Master)\\Courses\\D7043E - Advanced-Data-Mining\\U-Net project with Cell Images.ipynb Cell 2\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Docs/Sweden/Study/Amir/LTU/Applied%20AI%20%28Master%29/Courses/D7043E%20-%20Advanced-Data-Mining/U-Net%20project%20with%20Cell%20Images.ipynb#W1sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m x \u001b[39m=\u001b[39m up_conv_block(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Docs/Sweden/Study/Amir/LTU/Applied%20AI%20%28Master%29/Courses/D7043E%20-%20Advanced-Data-Mining/U-Net%20project%20with%20Cell%20Images.ipynb#W1sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m# Concatenate with the corresponding cropped feature map\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Docs/Sweden/Study/Amir/LTU/Applied%20AI%20%28Master%29/Courses/D7043E%20-%20Advanced-Data-Mining/U-Net%20project%20with%20Cell%20Images.ipynb#W1sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((x, encoder_outputs[reversed_layer]), dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Docs/Sweden/Study/Amir/LTU/Applied%20AI%20%28Master%29/Courses/D7043E%20-%20Advanced-Data-Mining/U-Net%20project%20with%20Cell%20Images.ipynb#W1sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m conv_block \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpansive_conv_blocks[layer]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Docs/Sweden/Study/Amir/LTU/Applied%20AI%20%28Master%29/Courses/D7043E%20-%20Advanced-Data-Mining/U-Net%20project%20with%20Cell%20Images.ipynb#W1sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m x \u001b[39m=\u001b[39m conv_block(x)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 112 but got size 120 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load and preprocess your input image\n",
    "image_path = './Datasets/Dataset 1/data/BMMC_1.tif'\n",
    "image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "input_image = transform(image).unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)\n",
    "\n",
    "# At this point, 'output' contains the model's prediction for the input image.\n",
    "# You can post-process the output as needed for your specific task.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
