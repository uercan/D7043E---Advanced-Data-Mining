{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we present the implementation of a U-Net architecture for semantic segmentation of cell images. The U-Net model is a popular choice for image segmentation tasks due to its ability to capture fine details and handle various object classes. Our goal is to segment cell images into three classes: Background, Cell Body, and Cell Kernel. We have implemented the U-Net architecture and designed a training pipeline to achieve this segmentation task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (contracting_conv_blocks): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (max_pooling_blocks): ModuleList(\n",
      "    (0-3): 4 x MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (connect): Sequential(\n",
      "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (expansive_conv_blocks): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (up_conv_blocks): ModuleList(\n",
      "    (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (3): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  )\n",
      "  (final): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.channels_list = [64, 128, 256, 512]\n",
    "        self.layers = len(self.channels_list)\n",
    "\n",
    "        # Contracting Part\n",
    "        self.contracting_conv_blocks = nn.ModuleList()\n",
    "        self.max_pooling_blocks = nn.ModuleList()\n",
    "        for channels in self.channels_list:\n",
    "            self.contracting_conv_blocks.append(self.conv_block(in_channels, channels))\n",
    "            self.max_pooling_blocks.append(nn.MaxPool2d(2))\n",
    "            in_channels = channels\n",
    "\n",
    "        # Connecting Part\n",
    "        self.connect = self.conv_block(\n",
    "            self.channels_list[-1], 2 * self.channels_list[-1]\n",
    "        )\n",
    "\n",
    "        # Expansive Part\n",
    "        in_channels = 1024\n",
    "        self.expansive_conv_blocks = nn.ModuleList()\n",
    "        self.up_conv_blocks = nn.ModuleList()\n",
    "        for channels in reversed(self.channels_list):\n",
    "            self.expansive_conv_blocks.append(self.conv_block(in_channels, channels))\n",
    "            self.up_conv_blocks.append(\n",
    "                nn.ConvTranspose2d(in_channels, channels, 2, stride=2)\n",
    "            )\n",
    "            in_channels = channels\n",
    "\n",
    "        # Final Part\n",
    "        self.final = nn.Conv2d(self.channels_list[0], out_channels, 1, padding=0)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def center_crop(self, x, crop_h, crop_w):\n",
    "        h, w = x.shape[2], x.shape[3]\n",
    "        y1 = (h - crop_h) // 2\n",
    "        y2 = y1 + crop_h\n",
    "        x1 = (w - crop_w) // 2\n",
    "        x2 = x1 + crop_w\n",
    "        return x[:, :, y1 : y2, x1 : x2]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Contracting Part\n",
    "        encoder_outputs = []\n",
    "        for layer in range(self.layers):\n",
    "            conv_block = self.contracting_conv_blocks[layer]\n",
    "            x = conv_block(x)\n",
    "            encoder_outputs.append(x)\n",
    "            max_pool = self.max_pooling_blocks[layer]\n",
    "            x = max_pool(x)\n",
    "\n",
    "        # Connecting Part\n",
    "        x = self.connect(x)\n",
    "\n",
    "        # Expansive Part\n",
    "        for reversed_layer in reversed(range(self.layers)):\n",
    "            layer = self.layers - reversed_layer - 1\n",
    "            up_conv_block = self.up_conv_blocks[layer]\n",
    "            x = up_conv_block(x)\n",
    "            # Concatenate with the corresponding feature map\n",
    "            # x = torch.cat([x, self.center_crop(encoder_outputs[reversed_layer], x.shape[2], x.shape[3])], dim=1)\n",
    "            # Since padding = 1 no need for cropping.\n",
    "            x = torch.cat([x, encoder_outputs[reversed_layer]], dim=1)\n",
    "\n",
    "            conv_block = self.expansive_conv_blocks[layer]\n",
    "            x = conv_block(x)\n",
    "\n",
    "        # Final Part\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create a U-Net model with 1 input channel and 3 output channels for segmentation\n",
    "model = UNet(in_channels=1, out_channels=3)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test input and output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 1024, 1024])\n",
      "Output shape: torch.Size([1, 3, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load and preprocess your input image\n",
    "image_path = './Datasets/Dataset 1/data/BMMC_1.tif'\n",
    "image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "input_image = transform(image).unsqueeze(0)  # Add a batch dimension\n",
    "print(\"Input shape:\", input_image.shape)\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)\n",
    "\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding code, we have defined the architecture of the U-Net model specifically tailored for the segmentation of cell images. The model is constructed with a contracting part to extract features and a corresponding expansive part to refine the segmentation masks. The connecting part serves as a bridge between these two parts, facilitating the flow of information across different scales.\n",
    "\n",
    "Our U-Net model is designed to work with cell images and output masks for three classes: Background, Cell Body, and Cell Kernel. It is worth noting that the model architecture has been implemented with an awareness of potential discrepancies in input and output sizes, which can arise due to zero padding in convolutional layers. The model is now ready for training with labeled data, enabling it to learn and improve its segmentation capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the U-Net model for image segmentation with three classes (Background, Cell Body, Cell Kernel), we'll need to define a dataset, a loss function, and an optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Average Loss: 1.6570890227029489\n",
      "Epoch [2/10] Average Loss: 0.5465780846601309\n",
      "Epoch [3/10] Average Loss: 0.5378157314519549\n",
      "Epoch [4/10] Average Loss: 0.5318459126838418\n",
      "Epoch [5/10] Average Loss: 0.551815826185914\n",
      "Epoch [6/10] Average Loss: 0.6284149425147578\n",
      "Epoch [7/10] Average Loss: 0.7424109825907752\n",
      "Epoch [8/10] Average Loss: 0.5301909853899202\n",
      "Epoch [9/10] Average Loss: 0.5323479395619658\n",
      "Epoch [10/10] Average Loss: 0.5396976027377817\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "# Define a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_files = os.listdir(image_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.image_files[idx] + \"_segmentation.tifnomalized.tif\")\n",
    "\n",
    "        image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "\n",
    "        # Open the mask image and convert it to an integer tensor\n",
    "        mask = Image.open(mask_path)\n",
    "        mask = mask.convert('L')  # Convert to grayscale\n",
    "        mask = np.array(mask) # Convert the PIL image to a NumPy array\n",
    "\n",
    "        # Convert the NumPy array to a PyTorch tensor with integer type\n",
    "        mask = torch.from_numpy(mask).to(torch.int64)\n",
    "\n",
    "        # Create a new tensor with the remapped values\n",
    "        mask = (mask == 85) * 0 + (mask == 170) * 1 + (mask == 255) * 2\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Define data directories\n",
    "image_dir = './Datasets/Dataset 1/data'\n",
    "mask_dir = './Datasets/Dataset 1/masks'\n",
    "\n",
    "# Define a transform to convert images and masks to tensors\n",
    "data_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Create a custom dataset\n",
    "dataset = CustomDataset(image_dir, mask_dir, transform=data_transform)\n",
    "\n",
    "# Create a dataloader\n",
    "batch_size = 1  # Adjust as needed\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define a loss function (e.g., cross-entropy) and an optimizer (e.g., Adam)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, masks in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs, masks = inputs.to(device), masks.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(f\"Epoch [{epoch + 1}/{num_epochs}] Current Loss: {loss.item()}\")\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # print()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Average Loss: {running_loss / len(dataloader)}\")\n",
    "    # print()\n",
    "    \n",
    "print(\"Training finished\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"unet_segmentation_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
